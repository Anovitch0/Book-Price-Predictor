"""
scrape_local_html.py
====================

This scraper reads a collection of local HTML files generated by
``generate_synthetic_books.py`` and extracts the book data into a
pandas DataFrame. It demonstrates how to use ``BeautifulSoup`` to
parse HTML tables. The script saves the consolidated data to a CSV
file for downstream machine learning tasks.

Usage:
    python scrape_local_html.py

The script assumes that the HTML files are located in
``scraping/local/pages`` relative to the project root and named
``page_1.html``, ``page_2.html``, etc.
"""

import csv
import os
from typing import List, Dict

from bs4 import BeautifulSoup
import pandas as pd

DATA_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), "data")
HTML_DIR = os.path.join(os.path.dirname(__file__), "local", "pages")
OUTPUT_CSV = os.path.join(DATA_DIR, "scraped_books_local.csv")


def parse_html_file(file_path: str) -> List[Dict[str, str]]:
    """Parse a single HTML file and return a list of record dictionaries."""
    with open(file_path, 'r', encoding='utf-8') as f:
        soup = BeautifulSoup(f, 'html.parser')

    table = soup.find('table')
    if table is None:
        return []

    # Extract header
    header_cells = table.find('thead').find_all('th')
    headers = [cell.get_text(strip=True) for cell in header_cells]

    records: List[Dict[str, str]] = []
    # Each row in tbody
    for row in table.find('tbody').find_all('tr'):
        cells = row.find_all(['td', 'th'])
        if not cells:
            continue
        values = [cell.get_text(strip=True) for cell in cells]
        record = dict(zip(headers, values))
        records.append(record)
    return records


def scrape_all_pages(html_dir: str) -> pd.DataFrame:
    """Iterate over all HTML files in ``html_dir`` and return a DataFrame."""
    records: List[Dict[str, str]] = []
    # Sort pages by page number to preserve order
    for filename in sorted(os.listdir(html_dir)):
        if not filename.endswith('.html'):
            continue
        file_path = os.path.join(html_dir, filename)
        page_records = parse_html_file(file_path)
        records.extend(page_records)
    df = pd.DataFrame(records)
    # Convert numeric columns
    numeric_cols = ['id', 'rating', 'price', 'availability']
    for col in numeric_cols:
        df[col] = pd.to_numeric(df[col], errors='coerce')
    return df


def main() -> None:
    df = scrape_all_pages(HTML_DIR)
    os.makedirs(DATA_DIR, exist_ok=True)
    df.to_csv(OUTPUT_CSV, index=False, quoting=csv.QUOTE_NONNUMERIC)
    print(f"Scraped {len(df)} records from local HTML pages.")
    print(f"Data saved to {OUTPUT_CSV}")


if __name__ == '__main__':
    main()